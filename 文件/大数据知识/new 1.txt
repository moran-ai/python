cmd中上传文件：scp 磁盘/路径/文件名@所传送地方的ip地址：文件所存放的目录（/home/hadoop)
scp E:\QQ文件\result_bigdata.txt root@192.168.128.200://opt/

切换路径:cd /usr/local/hadoop2.6.5


开启集群：sbin ./start-dfs.sh

开启监控：sbin ./start-yarn.sh

开启日志：sbin ./start-history.sever.sh

开启spark:sbin ./start-all.sh       /usr/local/spark-2.2.2-bin-hadoop2.6/sbin/start-all.sh

打开spark终端：./spark-shell     先切换到hadoop的bin/目录下

进入数据库：mysql -uhive -p      密码：hive

关闭集群：sbin ./stop-dfs.sh

关闭监控：sbin .stop-yarn.sh

查看集群数据节点：hdfs dfsadmin -report

Hdfs中创建新目录：hdfs dfs -mkdir [-p] <path>    -p:不加只能逐级创建目录，加了可以多级创建目录

cd/home 进入'/home'目录

cd .. 返回上一级目录

cd ../.. 返回上两级目录

cd 进入个人的主目录

cd ~ 切换到用户的主目录

cd ~user1 进入个人的主目录

cd - 返回上次所在的目录

pwd  显示工作路径

ll 显示当前目录下文件的详细信息

ls 查看目录中的文件

ls -F 查看目录中的文件

ls -l 显示文件和目录的详细资料

ls -a 显示隐藏文件

ls *[0-9]* 显示包含数字的文件名和目录名

tree 显示文件和目录由根目录开始的树形结构

lstree 显示文件和目录由根目录开始的树形结构

mkdir dir1 创建一个叫做dir1的目录

mkdir dir1 dir2 同时创建两个目录

mkdir -p/tmp/dir1/dir2 创建一个目录树

将Spark安装包分发到其他节点
scp -r /usr/local/spark-2.2.2-bin-hadoop2.6/ node201:/usr/local/   分到node201节点
scp -r /usr/local/spark-2.2.2-bin-hadoop2.6/ node202:/usr/local/   分到node202节点
scp -r /usr/local/spark-2.2.2-bin-hadoop2.6/ node203:/usr/local/   分到node203节点


掌握DataFrame基础操作
1.外部数据库创建DataFrame
* 进入数据库
* 创建数据库test
* 创建数据表people
* 在数据表中加入一条记录
* 退出数据库
* 进入scala
（使用这种方式创建DataFrame需要通过JDBC连接或ODBC连接的方式访问数据库）

* 在Scala中输入val dfJDBC = spark.sqlContext.read.format("jdbc").options(   
     | Map("url"->url,
     | "user"->"hive",
     |  "password"->"hive",
     | "dbtable"->"people")).load()
* 返回结果：dfJDBC: org.apache.spark.sql.DataFrame = [id: int, name: string]

* 显示结果：dfJDBC.show
+---+-----+
| id| name|
+---+-----+
|  3|tobmy|
+---+-----+




在cmd中将person.txt文件传到//opt/下
将txt数据读入Hive数据库：load data local inpath '/opt/person.txt' overwrite into table person;


spark.sql("use test")  读取数据库中的库test




设置日志级别：
org.apache.log4j.Logger.getLogger("org").setLevel(org.apache.log4j.Level.WARN)
org.apache.log4j.Logger.getLogger("org").setLevel(org.apache.log4j.Level.WARN)

scala> sc.setLogLevel("WARN")
scala> sc.setLogLevel("INFO")


创建RDD的方式：
1.从集合中创建RDD：
parallelize():通过parallelize函数把一般数据加载为RDD
val list = List(1,2,3,4)
val rdd1 = sc.parallelize(list)
rdd1.partitions.size
val rdd1 = sc.parallelize(list,2)
rdd1.partitions.size

makeRDD():通过parallelize函数把一般数据结构加载为RDD
val list=sc.makeRDD(List("a","b","c","d"))
list.partitions.size
val list1=sc.makeRDD(List("a","b","c","d"),2)
list1.partitions.size




2.从外部存储创建RDD
* 通过textFile直接加载数据文件为RDD
val loaddata = sc.text.File("file:///opt/words.txt")


上传文件到目录：hdfs dfs -put /opt/words.txt /user/root
(1)将本地文件加载到rdd，并查看
val rdd_file = sc.textFile("file:///opt/words.txt")
rdd_file.collect

(2)将hdfs上文件加载到rdd，并查看
val rdd3_file = sc.textFile("/user/root/words.txt")
rdd3_file.collect


（1）将student.txt上传到node200上的/opt目录下
（2）将student.txt上传到hdfs上
     root@node200:/opt# hdfs dfs -put student.txt /user/root/
（3）val rdd4_file = sc.textFile("/user/root/student.txt")
     rdd4_file.collect



将数据上传到HDFS文件系统
hdfs dfs -put studet.txt /user/root
hdfs dfs -put result_bigdata /user/root
hdfs dfs -put result_math /user/root



RDD转换与操作
1.使用map函数对RDD中每个元素进行平方操作
val square=data.map(x=>x*x)

2.使用map函数产生键值对RDD
data.map(x=>(x,1))

3.使用flatMap对集合中的每个元素进行操作再扁平化
data.flatMap(x=>x.split(" "))

4.使用sortBy()对标准RDD进行排序
sortBy()可以接受三个参数
第一个参数是一个函数f:(T) => K，左边是要被排序对象中的每一个元素，右边返回的值是元素中要进行排序的值。
第二个参数是ascending，决定排序后RDD中的元素是升序还是降序，默认是true，也就是升序
第三个参数是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等，即为this.partitions.size。


5.filter:
* 对元素进行过滤，对每个元素应用给定的函数，返回值为true的元素在RDD中保留，为false的则被过滤掉；
* 过滤掉data RDD中元素小于或等于2的元素


6.distinct：去重
针对RDD中重复的元素，只保留一个元素
data.distinct.collect


7.union
* 合并RDD，需要保证两个RDD元素类型一致

8.subtract:
该方法是在两个RDD间进行的，其主要获取两个RDD之间的差集
subtractRDD.collect

9.intersection
找出两个RDD的共同元素，也就是找出两个RDD的交集
c_rdd1.intersection(c_rdd2).collect

10.cartesian:笛卡尔积就是将两个集合的元素两两组合成一组
rdd01.cartersian(rdd02).collect


键值对RDD
* 创建键值对:有很多种创建键值对RDD的方式，很多存储键值对的数据格式会在读取时直接返回由其键值对组成的PairRDD。当需要将一个普通的RDD转化为一个PairRDD时可以使用map函数来进行操作，传递的函数需要返回键值对。
* 做为键值对类型的RDD，包含了键跟值两个部分。Spark提供了两个方法分别获取键值对RDD的键跟值。keys返回一个仅包含键的RDD，values返回一个仅包含值的RDD。

mapValues
mapValues是针对键值对（Key，Value）类型的数据中的Value进行Map操作，而不对Key进行处理















